Ampligraph RequirementsTo run AmpliGraph, you need to install the following dependencies:1. NumPy version: 1.23.4: A fundamental package for scientific computing with Python.pip install numpy 2. AmpliGraph version: 2.1.0: The main library for knowledge graph embedding.pip install ampligraph 3. TensorFlow version: 2.9.0: A deep learning framework used as a backend for AmpliGraph.pip install tensorflow 4. Optuna version: 3.6.0: A hyperparameter optimization framework used for tuning model parameters.pip install optuna 5. Wandb version: 0.16.5: A tool for experiment tracking, hyperparameter optimization, pip install wandb 6. Joblib version: 1.2.0: A set of tools to provide lightweight pipelining in Python.pip install joblib7. optuna_integration: A hyperparameter optimization framework used for tuning model parameters for integration. # AmpliGraph Hyperparameter Optimization Documentation## OverviewThis script is designed for hyperparameter optimization (HPO) of a knowledge graph embedding model using Optuna and Weights and Biases (wandb) for experiment tracking.## DependenciesEnsure you have the following dependencies installed:- ampligraph- optuna- wandb- numpy- tensorflow## Configuration Parameters- `WANDB_PROJECT`: Name of the Weights and Biases project.- `WANDB_ENTITY`: Weights and Biases entity.- `MODEL_NAME`: Name of the knowledge graph embedding model (e.g., TransE).- `WANDB_API_KEY`: API key for Weights and Biases authentication.- `NUM_TRIALS`: Number of optimization trials to run.## Training Procedure### Loading the DatasetThe FB15k-237 dataset is loaded using AmpliGraph.### Model InitializationThe ScoringBasedEmbeddingModel is initialized with specific parameters like embedding size (k), corruption count (eta), and scoring type (MODEL_NAME).### Hyperparameter OptimizationHyperparameters such as loss function type, optimizer type, learning rate, and batch size are optimized using Optuna.### Model CompilationThe model is compiled with the specified optimizer, loss function, and entity-relation regularizer.### Training and ValidationThe training process includes early stopping and validation. Validation is performed every 20 epochs.### EvaluationEvaluation includes computing ranks and calculating metrics like MRR and Hits@10.### LoggingExperiment metrics are logged using Weights and Biases.## Optimization ProcedureA Optuna study is created with a specific name, direction (maximize), and sampler (TPESampler). The study is optimized by calling the `train_main_optuna` function for a specified number of trials.## OutputThe output includes the best hyperparameters found during optimization and the study object containing trial information.## UsageTo run the script, ensure all dependencies are installed and configure the necessary parameters in the script. Then, execute the script in your preferred Python environment.## References- AmpliGraph Documentation: [link](https://docs.ampligraph.org)- Optuna Documentation: [link](https://optuna.readthedocs.io/en/stable/)- Weights and Biases Documentation: [link](https://docs.wandb.ai/)